init_args: 
  _target_: models.generators.llm_cocom.COCOMLLM
  model_name: "/beegfs/scratch/user/hdejean/calmar/dec_from_emb/experiments_compress_real/4330703/last_model"
  decoder_model_name: "mistralai/Mistral-7B-Instruct-v0.2"
  compr_model_name: null
  max_new_tokens: 128
  sep: True
  compr_rate: 4
  compr_linear_type: 'concat'
  quantization: 'no'
  generation_top_k: ${generation_top_k}
  context_max_length: 128
batch_size: 4
