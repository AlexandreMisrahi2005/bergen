run_name: null
dataset_folder: datasets/
index_folder: indexes/
runs_folder: runs/
generated_query_folder: generated_queries/
experiments_folder: experiments/
retrieve_top_k: 50
rerank_top_k: 50
generation_top_k: 5
pyserini_num_threads: 20
processing_num_proc: 40
retriever:
  init_args:
    _target_: models.retrievers.bm25.BM25
    model_name: bm25
  batch_size: 512
  batch_size_sim: 2048
reranker:
  init_args:
    _target_: models.rerankers.crossencoder.CrossEncoder
    model_name: cross-encoder/ms-marco-MiniLM-L-6-v2
  batch_size: 2048
generator:
  init_args:
    _target_: models.generators.llm.LLM
    model_name: TinyLlama/TinyLlama-1.1B-Chat-v1.0
    max_new_tokens: 64
    max_length: 2048
    quantization: int4
    attn_implementation: sdpa
    batch_size: 1
dataset:
  train:
    doc:
      init_args:
        _target_: modules.dataset_processor.KILT100w
        split: full
    query:
      init_args:
        _target_: modules.dataset_processor.KILTNQ
        split: train
  dev:
    doc:
      init_args:
        _target_: modules.dataset_processor.KILT100w
        split: full
    query:
      init_args:
        _target_: modules.dataset_processor.KILTNQ
        split: validation
  test:
    doc: null
    query: null
prompt:
  system: You are a helpful assistant. Your task is to extract relevant information
    from provided documents and to answer to questions as briefly as possible.
  user: f"Background:\n{docs}\n\nQuestion:\ {question}"
  system_without_docs: You are a helpful assistant. Answer the questions as briefly
    as possible.
  user_without_docs: f"Question:\ {question}"
