[2024-07-31 12:43:48,010][datasets][INFO] - PyTorch version 2.3.1 available.
[2024-07-31 12:43:48,013][datasets][INFO] - TensorFlow version 2.8.0 available.
Unfinished experiment_folder: testbergen/tmp_bioasq_pubmed_ragged_splade-v3
experiment_folder testbergen/bioasq_pubmed_ragged_splade-v3
run_name: bioasq_pubmed_ragged_splade-v3
dataset_folder: datasets/
index_folder: indexes/
runs_folder: runs/
generated_query_folder: generated_queries/
experiments_folder: testbergen
retrieve_top_k: 50
rerank_top_k: 50
generation_top_k: 5
pyserini_num_threads: 20
processing_num_proc: 40
retriever:
  init_args:
    _target_: models.retrievers.splade.Splade
    model_name: naver/splade-v3
    max_len: 128
  batch_size: 512
  batch_size_sim: 512
generator:
  init_args:
    _target_: models.generators.vllm.LLM
    model_name: Upstage/SOLAR-10.7B-Instruct-v1.0
    max_new_tokens: 128
    max_length: 4096
    batch_size: 256
dataset:
  dev:
    doc:
      init_args:
        _target_: modules.dataset_processor.PubMed2023_Ragged
        split: train
    query:
      init_args:
        _target_: modules.dataset_processor.BIOASQ11B_Ragged
        split: train
  test:
    doc: null
    query: null
prompt:
  system: You are a helpful assistant. Your task is to extract relevant information
    from provided documents and to answer to questions as briefly as possible.
  user: f"Background:\n{docs}\n\nQuestion:\ {question}"
  system_without_docs: You are a helpful assistant. Answer the questions as briefly
    as possible.
  user_without_docs: f"Question:\ {question}"

Processing dataset PubMed-2023_Ragged in train split 
Processing dataset BIOASQ11B_Ragged in train split 
Checking dataset..:   0%|          | 0/3837 [00:00<?, ?it/s]Checking dataset..:  93%|█████████▎| 3571/3837 [00:00<00:00, 35703.73it/s]Checking dataset..: 100%|██████████| 3837/3837 [00:00<00:00, 35567.81it/s]
INFO 07-31 12:45:05 config.py:472] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor
INFO 07-31 12:45:05 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='Upstage/SOLAR-10.7B-Instruct-v1.0', speculative_config=None, tokenizer='Upstage/SOLAR-10.7B-Instruct-v1.0', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=fp8, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=Upstage/SOLAR-10.7B-Instruct-v1.0, use_v2_block_manager=False, enable_prefix_caching=False)
INFO 07-31 12:45:09 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 07-31 12:45:09 selector.py:54] Using XFormers backend.
INFO 07-31 12:45:10 model_runner.py:680] Starting to load model Upstage/SOLAR-10.7B-Instruct-v1.0...
INFO 07-31 12:45:10 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 07-31 12:45:10 selector.py:54] Using XFormers backend.
INFO 07-31 12:45:12 weight_utils.py:223] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:03<00:13,  3.46s/it]
Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:07<00:10,  3.58s/it]
Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:08<00:05,  2.54s/it]
Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:11<00:02,  2.92s/it]
Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:15<00:00,  3.15s/it]
Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:15<00:00,  3.10s/it]

INFO 07-31 12:45:30 model_runner.py:692] Loading model weights took 19.9900 GB
INFO 07-31 12:45:32 gpu_executor.py:102] # GPU blocks: 5012, # CPU blocks: 2730


::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
RAG Model:
Retriever: naver/splade-v3
Generator: Upstage/SOLAR-10.7B-Instruct-v1.0
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::


Run runs//run.retrieve.top_50.BIOASQ11B_Ragged.PubMed-2023_Ragged.dev.naver_splade-v3.trec does not exists, running retrieve...
Encoding: naver/splade-v3:   0%|          | 0/8 [00:00<?, ?it/s]Encoding: naver/splade-v3:  12%|█▎        | 1/8 [00:00<00:03,  1.81it/s]Encoding: naver/splade-v3:  12%|█▎        | 1/8 [00:00<00:05,  1.20it/s]
Error executing job with overrides: ['retriever=splade-v3', 'generator=vllm_SOLAR-107B', 'dataset=pubmed_bioasq', '++experiments_folder=testbergen', '+run_name=bioasq_pubmed_ragged_splade-v3']
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/amisrahi/bergen/bergen.py", line 29, in <module>
[rank0]:     main()
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
[rank0]:     _run_hydra(
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
[rank0]:     _run_app(
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
[rank0]:     run_and_report(
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
[rank0]:     raise ex
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
[rank0]:     return func()
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
[rank0]:     lambda: hydra.run(
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 132, in run
[rank0]:     _ = ret.return_value
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/hydra/core/utils.py", line 260, in return_value
[rank0]:     raise self._return_value
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
[rank0]:     ret.return_value = task_function(task_cfg)
[rank0]:   File "/home/amisrahi/bergen/bergen.py", line 23, in main
[rank0]:     rag.eval(dataset_split='dev')
[rank0]:   File "/home/amisrahi/bergen/modules/rag.py", line 157, in eval
[rank0]:     query_ids, doc_ids, _ = self.retrieve(
[rank0]:   File "/home/amisrahi/bergen/modules/rag.py", line 246, in retrieve
[rank0]:     out_ranking = self.retriever.retrieve(
[rank0]:   File "/home/amisrahi/bergen/modules/retrieve.py", line 54, in retrieve
[rank0]:     self.index(dataset, query_embeds_path, query_or_doc='query', overwrite_index=overwrite_index)
[rank0]:   File "/home/amisrahi/bergen/modules/retrieve.py", line 48, in index
[rank0]:     _ = self.encode_and_save(dataset, save_path=index_path, query_or_doc=query_or_doc)
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/amisrahi/bergen/modules/retrieve.py", line 135, in encode_and_save
[rank0]:     outputs = self.model(query_or_doc, batch)
[rank0]:   File "/home/amisrahi/bergen/models/retrievers/splade.py", line 41, in __call__
[rank0]:     outputs = self.model(**kwargs).logits
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py", line 1506, in forward
[rank0]:     prediction_scores = self.cls(sequence_output)
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py", line 797, in forward
[rank0]:     prediction_scores = self.predictions(sequence_output)
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py", line 787, in forward
[rank0]:     hidden_states = self.decoder(hidden_states)
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 116, in forward
[rank0]:     return F.linear(input, self.weight, self.bias)
[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.28 GiB. GPU 
