[2024-07-31 13:45:33,545][datasets][INFO] - PyTorch version 2.3.1 available.
[2024-07-31 13:45:33,547][datasets][INFO] - TensorFlow version 2.8.0 available.
Unfinished experiment_folder: testbergen/tmp_bioasq_pubmed_ragged_splade-v3
experiment_folder testbergen/bioasq_pubmed_ragged_splade-v3
run_name: bioasq_pubmed_ragged_splade-v3
dataset_folder: datasets/
index_folder: indexes/
runs_folder: runs/
generated_query_folder: generated_queries/
experiments_folder: testbergen
retrieve_top_k: 50
rerank_top_k: 50
generation_top_k: 5
pyserini_num_threads: 20
processing_num_proc: 40
retriever:
  init_args:
    _target_: models.retrievers.splade.Splade
    model_name: naver/splade-v3
    max_len: 512
  batch_size: 128
  batch_size_sim: 512
generator:
  init_args:
    _target_: models.generators.vllm.LLM
    model_name: Upstage/SOLAR-10.7B-Instruct-v1.0
    max_new_tokens: 128
    max_length: 4096
    batch_size: 256
dataset:
  dev:
    doc:
      init_args:
        _target_: modules.dataset_processor.PubMed2023_Ragged
        split: train
    query:
      init_args:
        _target_: modules.dataset_processor.BIOASQ11B_Ragged
        split: train
  test:
    doc: null
    query: null
prompt:
  system: You are a helpful assistant. Your task is to extract relevant information
    from provided documents and to answer to questions as briefly as possible.
  user: f"Background:\n{docs}\n\nQuestion:\ {question}"
  system_without_docs: You are a helpful assistant. Answer the questions as briefly
    as possible.
  user_without_docs: f"Question:\ {question}"

Processing dataset PubMed-2023_Ragged in train split 
Processing dataset BIOASQ11B_Ragged in train split 
Checking dataset..:   0%|          | 0/3837 [00:00<?, ?it/s]Checking dataset..:  95%|█████████▍| 3644/3837 [00:00<00:00, 36434.65it/s]Checking dataset..: 100%|██████████| 3837/3837 [00:00<00:00, 36312.07it/s]
INFO 07-31 13:46:15 config.py:472] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor
INFO 07-31 13:46:15 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='Upstage/SOLAR-10.7B-Instruct-v1.0', speculative_config=None, tokenizer='Upstage/SOLAR-10.7B-Instruct-v1.0', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=fp8, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=Upstage/SOLAR-10.7B-Instruct-v1.0, use_v2_block_manager=False, enable_prefix_caching=False)
INFO 07-31 13:46:16 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 07-31 13:46:16 selector.py:54] Using XFormers backend.
INFO 07-31 13:46:17 model_runner.py:680] Starting to load model Upstage/SOLAR-10.7B-Instruct-v1.0...
INFO 07-31 13:46:17 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 07-31 13:46:17 selector.py:54] Using XFormers backend.
INFO 07-31 13:46:18 weight_utils.py:223] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:03<00:13,  3.48s/it]
Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:07<00:10,  3.52s/it]
Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:08<00:04,  2.48s/it]
Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:11<00:02,  2.85s/it]
Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:15<00:00,  3.12s/it]
Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:15<00:00,  3.06s/it]

INFO 07-31 13:46:34 model_runner.py:692] Loading model weights took 19.9900 GB
INFO 07-31 13:46:36 gpu_executor.py:102] # GPU blocks: 5012, # CPU blocks: 2730


::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
RAG Model:
Retriever: naver/splade-v3
Generator: Upstage/SOLAR-10.7B-Instruct-v1.0
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::


Run runs//run.retrieve.top_50.BIOASQ11B_Ragged.PubMed-2023_Ragged.dev.naver_splade-v3.trec does not exists, running retrieve...
Load embeddings...: 0it [00:00, ?it/s]Load embeddings...: 0it [00:00, ?it/s]
Error executing job with overrides: ['retriever=splade-v3', 'generator=vllm_SOLAR-107B', 'dataset=pubmed_bioasq', '++experiments_folder=testbergen', '+run_name=bioasq_pubmed_ragged_splade-v3']
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/amisrahi/bergen/utils.py", line 43, in load_embeddings
[rank0]:     embeds = torch.concat(embeds)
[rank0]: RuntimeError: torch.cat(): expected a non-empty list of Tensors

[rank0]: During handling of the above exception, another exception occurred:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/amisrahi/bergen/bergen.py", line 29, in <module>
[rank0]:     main()
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
[rank0]:     _run_hydra(
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
[rank0]:     _run_app(
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
[rank0]:     run_and_report(
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
[rank0]:     raise ex
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
[rank0]:     return func()
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
[rank0]:     lambda: hydra.run(
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 132, in run
[rank0]:     _ = ret.return_value
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/hydra/core/utils.py", line 260, in return_value
[rank0]:     raise self._return_value
[rank0]:   File "/beegfs/scratch/user/amisrahi/miniconda3/envs/bergen/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
[rank0]:     ret.return_value = task_function(task_cfg)
[rank0]:   File "/home/amisrahi/bergen/bergen.py", line 23, in main
[rank0]:     rag.eval(dataset_split='dev')
[rank0]:   File "/home/amisrahi/bergen/modules/rag.py", line 157, in eval
[rank0]:     query_ids, doc_ids, _ = self.retrieve(
[rank0]:   File "/home/amisrahi/bergen/modules/rag.py", line 246, in retrieve
[rank0]:     out_ranking = self.retriever.retrieve(
[rank0]:   File "/home/amisrahi/bergen/modules/retrieve.py", line 74, in retrieve
[rank0]:     query_embeds = load_embeddings(query_embeds_path)
[rank0]:   File "/home/amisrahi/bergen/utils.py", line 45, in load_embeddings
[rank0]:     raise IOError(f'Embedding index corrupt. Please delete folder "{index_path}" and run again.')
[rank0]: OSError: Embedding index corrupt. Please delete folder "indexes/BIOASQ11B_Ragged_dev_query_naver_splade-v3" and run again.
